{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing Neccessary Dependencies"
      ],
      "metadata": {
        "id": "yhmizltqTYo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxgIraWQ1IGI",
        "outputId": "a5326809-025f-4c7b-fa97-f07aed905ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for librosa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, lavfi, from 'anullsrc=r=44100:cl=mono':\n",
            "  Duration: N/A, start: 0.000000, bitrate: 352 kb/s\n",
            "    Stream #0:0: Audio: pcm_u8, 44100 Hz, mono, u8, 352 kb/s\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (pcm_u8 (native) -> mp3 (libmp3lame))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp3, to 'Temp.mp3':\n",
            "  Metadata:\n",
            "    TSSE            : Lavf58.29.100\n",
            "    Stream #0:0: Audio: mp3 (libmp3lame), 44100 Hz, mono, s16p\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libmp3lame\n",
            "size=      39kB time=00:00:10.00 bitrate=  32.1kbits/s speed= 226x    \n",
            "video:0kB audio:39kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.568409%\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q openai\n",
        "!pip install -q gTTS\n",
        "!pip install -q git+https://github.com/librosa/librosa\n",
        "!pip install -q pydub\n",
        "!pip install -q resampy\n",
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Notebook"
      ],
      "metadata": {
        "id": "llCupWJDT-Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the libraries"
      ],
      "metadata": {
        "id": "oiCre9BEUPuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import gradio as gr \n",
        "import time\n",
        "import warnings\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "from gtts import gTTS\n",
        "from google.colab import drive\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import math\n",
        "import librosa\n",
        "import resampy\n",
        "from pydub import AudioSegment\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "_jNHgv_B1Q7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1535f7f2-2d1c-4f32-ed07-3e255430e330"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup files path for Colab to access"
      ],
      "metadata": {
        "id": "-EVLiMYEUeP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/gdrive/MyDrive/Colab_Notebooks'\n",
        "os.chdir(main_path)"
      ],
      "metadata": {
        "id": "YyrLNS06Zu2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_enc = {'fear':0, 'disgust':1, 'neutral':2, 'happy':3, 'sadness':4, 'surprise':5, 'angry':6}\n",
        "emotion_dec = {v: k for k, v in emotion_enc.items()}"
      ],
      "metadata": {
        "id": "T-lXDALQa1HK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xi3Tj5XM1Vfn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '' #Insert OpenAI API Key Here"
      ],
      "metadata": {
        "id": "QsgKsiT01XCj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\") #or base\n",
        "model.device"
      ],
      "metadata": {
        "id": "Rm-112_11XLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4666c4f-b4ae-4efd-cd5c-b15eb40f707a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 121MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "b80GMVH_U7fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function to generate system response with the help of OpenAI's GPT4 API"
      ],
      "metadata": {
        "id": "E7gXJw_-VB3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_api(input_text, emotion_result):\n",
        "    system_prompt = \"User's emotion is \" + emotion_result + \" , as a Therapist utilize this for any question the user may have and help him by keeping the emotion in mind.\"\n",
        "    print(system_prompt)\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt}]\n",
        "    \n",
        "    if input_text:\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": input_text},\n",
        "        )\n",
        "        chat_completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", messages=messages\n",
        "        )\n",
        "    \n",
        "    reply = chat_completion.choices[0].message.content\n",
        "    return reply"
      ],
      "metadata": {
        "id": "rXiog8uV1nVs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function to detect user's emotion"
      ],
      "metadata": {
        "id": "5fb3zZ1oVVXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discover_emotion(audio):\n",
        "    input_file = audio\n",
        "    x,sr = librosa.load(input_file,\n",
        "             res_type='kaiser_fast', sr=44000)\n",
        "    length_chosen = 120378\n",
        "    if x.shape[0] > length_chosen:\n",
        "        new = x[:length_chosen]\n",
        "    elif x.shape[0] < length_chosen:\n",
        "        new = np.pad(x,math.ceil((length_chosen-x.shape[0])/2), mode='median')\n",
        "    else:\n",
        "        new = x\n",
        "    mfcc = librosa.feature.mfcc(y=new, sr=44000, n_mfcc=40)\n",
        "    mfcc = mfcc.T\n",
        "    mfcc.shape\n",
        "    emotion_model = load_model(\"model3.h5\")\n",
        "    mfcc = mfcc.reshape(1,236,40)\n",
        "    mfcc.shape\n",
        "    p = emotion_model.predict(mfcc)\n",
        "    emotion_result = emotion_dec[p.argmax()]\n",
        "\n",
        "    return emotion_result\n"
      ],
      "metadata": {
        "id": "LpgxsgvWVdkQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function for voice to text transcription(Whisper) and text to voice generation(HuggingFace TTS)"
      ],
      "metadata": {
        "id": "vHM3nHz4WzgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio):\n",
        "    language = 'en'\n",
        "\n",
        "    #call emotion recognition\n",
        "    emotion_result = discover_emotion(audio)\n",
        "    #Chatbot \n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "    \n",
        "    out_result = chatgpt_api(result_text, emotion_result)\n",
        "    \n",
        "    audioobj = gTTS(text = out_result, \n",
        "                    lang = language, \n",
        "                    slow = False)\n",
        "    \n",
        "    audioobj.save(\"Temp.mp3\")\n",
        "\n",
        "    return [result_text,emotion_result, out_result, \"Temp.mp3\"]\n",
        "  "
      ],
      "metadata": {
        "id": "TbZgdyrvcXG-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Gradio intrface for user to interact with the chatbot"
      ],
      "metadata": {
        "id": "DpfyZ2EiW4rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_1 = gr.Textbox(label=\"Speech to Text\")\n",
        "output_2 = gr.Textbox(label=\"ChatGPT Output\")\n",
        "output_3 = gr.Audio(\"Temp.mp3\")\n",
        "output_4 = gr.Textbox(label=\"User Emotion Detected\")\n",
        "gr.Interface(\n",
        "    title = 'EmotionGPT', \n",
        "    fn=transcribe, \n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
        "    ],\n",
        "\n",
        "    outputs=[\n",
        "        output_1, output_4, output_2, output_3 \n",
        "    ],\n",
        "    live=True).launch(debug=True)\n",
        "# inputFile= \"hello.mp3\"\n",
        "# transcribe(inputFile)"
      ],
      "metadata": {
        "id": "ien-eK7B1neo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "ac5a3f3e-0df9-4331-b41b-02bba472a983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step\n",
            "User's emotion is neutral , as a Therapist utilize this for any question the user may have and help him by keeping the emotion in mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyH9o_SkdeJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}